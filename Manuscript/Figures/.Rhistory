indics_cat_qq<-paste0(indics,'_q')
quant.cut<-function(var,x,df){
xvec<-vector()
for (i in 1:x){
xvec[i]<-i/x
}
qs<-c(min(df[[var]],na.rm=T), quantile(df[[var]],xvec,na.rm=T))
df[['new']]=x+1 #initialize variable
for (i in 1:(x)){
df[['new']]<-ifelse(df[[var]]<qs[i+1] & df[[var]]>=qs[i],
c(1:length(qs))[i],
ifelse(df[[var]]==qs[qs==max(qs)],x,df[['new']]))
}
return(df[['new']])
}
for(i in 1:length(indics_cat_qq)){
x.dataca[,indics_cat_qq[i]]<-quant.cut(var = indics[i],
x=5,df=x.dataca)
}
trend.func<-function(rank.var,cont.var,df,trend.var,x){
df[[trend.var]]=1
medians<-vector()
for (i in 1:x){
newdf<-df[df[[rank.var]]==i,]
medians[i]<-median(newdf[[cont.var]],na.rm=T)
df[[trend.var]]<-ifelse(df[[rank.var]]==i,medians[i],df[[trend.var]])
}
return(df)
}
indics_cat_trend<-paste0(indics,'_trend')
for (i in 1:length(indics_cat_trend)){
x.dataca<-trend.func(rank.var=indics_cat_qq[i],
cont.var=indics[i],df=x.dataca, trend.var=indics_cat_trend[i],x=5)
}
# create scaled variables for modeling as continuous variables
indics_cat_scale<-paste0(indics,'_sc')
indics_cat_scale_sq<-paste0(indics,'_sc2')# squared
new.subs.cond.v2<-str_replace_all(new.subs.cond,'datf','x.dataca')
for(i in 1:length(indics_cat_scale)){
x.dataca[,indics_cat_scale[i]]<-x.dataca[,indics[i]]/sd(x.dataca[which(eval(parse(text=new.subs.cond.v2))),indics[i]],na.rm=T)
x.dataca[,indics_cat_scale_sq[i]]<-(x.dataca[,indics[i]]/sd(x.dataca[which(eval(parse(text=new.subs.cond.v2))),indics[i]],na.rm=T))^2
}
keep.columns<-which(colnames(x.dataca) %in% c('SEQN','FS_ENet_q','Age_ENet_q',
'FdAs_ENet_q','HHS_ENet_q','PC1_q','PC2_q',
'FS_ENet_trend','Age_ENet_trend',
'FdAs_ENet_trend','HHS_ENet_trend','PC1_trend','PC2_trend',
'FS_ENet_sc','Age_ENet_sc',
'FdAs_ENet_sc','HHS_ENet_sc','PC1_sc','PC2_sc',
'FS_ENet_sc2','Age_ENet_sc2',
'FdAs_ENet_sc2','HHS_ENet_sc2','PC1_sc2','PC2_sc2'))
x.data4<-left_join(datf,
x.dataca[,keep.columns],by='SEQN')
x.data5<-x.data4[which(is.na(x.data4$WTDR14YR)==F),]
## Survival
# Design object
mod.design<-svydesign(id = ~SDMVPSU, weights = ~WTDR14YR, strata = ~SDMVSTRA,
nest = TRUE, survey.lonely.psu = "adjust", data = x.data5)
mod.design<-subset(mod.design,eval(parse(text=subset.condition)))#inclusions
# Fit models and generate table of results
model.list<-list()
i=1
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars,collapse = '+'))),design=mod.design)
indics[i]
covars
table(x.data5$FS_ENet_q)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~',
paste0(covars,collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-1],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-2],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-3],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-4],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-5],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-6],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-7],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-8],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+')),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)')),design=mod.design)
View(modelfull)
modelfull$coefficients
summary(modelfull)
covars
x.data5[,covars]
covars
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-8],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(5:8)],collapse = '+'))),design=mod.design)
length(covars)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(5:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(6:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(7:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(8:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(9:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(10:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(11:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(12:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(13:14)],collapse = '+'))),design=mod.design)
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(14:14)],collapse = '+'))),design=mod.design)
14:14
covars[14]
modelfull<-svycoxph(formula(paste0('Surv(stime,',censor,')~factor(',indics[i],'_q)+',
paste0(covars[-c(13)],collapse = '+'))),design=mod.design)
summary(modelfull)
wd<- ( '/Volumes/My Passport for Mac/Urban Institute/Summer Projects/Geospatial Dashboard/np-density-dashboard/Data-Wrangled' )
setwd( wd )
# read in ppl data
setwd( wd )
ppl <- readRDS( "pplAddresses_census.rds" )
# setting wd
wd2 <- '/Volumes/My Passport for Mac/Urban Institute/Summer Projects/Geospatial Dashboard/np-density-dashboard/addresses_ppl'
setwd( wd2 )
# Selecting only essential variables
ppl <- dplyr::select( ppl, ID, Address, City, State, Zip )
# Splitting address files into files with 500 addresses each
loops <- ceiling( nrow( ppl ) / 500 ) # ceiling function rounds up an integer. so loops has the amount of 500s that fit rounded up.
loops
# setting wd
setwd( wd2 )
#left off at 1826
for ( i in 1826:loops ){
start_time <- Sys.time( )# document start times
res<- tibble( read.csv( paste0( "Addressppl", i, ".csv" ) ) )%>%
geocode( ., street = Address, city = City, state = State, postalcode = Zip, method = 'census', full_results = T )
write.csv( res, paste0( "Results/Resultsppl", i, ".csv" ), row.names = F )
end_time <- Sys.time( )# document start times
print( paste0( "Iteration #", i, " complete" ) ) # print iteration to keep track of loop
print( end_time - start_time ) # print system time to keep track of iteration progress
}
library( tidyverse )
library( tidygeocoder ) # for geocoding addresses using Census API, Google API, etc.
# setting wd
setwd( wd2 )
#left off at 1826
for ( i in 1826:loops ){
start_time <- Sys.time( )# document start times
res<- tibble( read.csv( paste0( "Addressppl", i, ".csv" ) ) )%>%
geocode( ., street = Address, city = City, state = State, postalcode = Zip, method = 'census', full_results = T )
write.csv( res, paste0( "Results/Resultsppl", i, ".csv" ), row.names = F )
end_time <- Sys.time( )# document start times
print( paste0( "Iteration #", i, " complete" ) ) # print iteration to keep track of loop
print( end_time - start_time ) # print system time to keep track of iteration progress
}
setwd("/Volumes/My Passport for Mac/Arthur Lab/FPED Raw Data/Analysis files/GitHub Repository Files /NHANES-Diet-Penalized-Regression/Data-Wrangled")
xdata<-readRDS("03-Inclusions-Exclusions.rds")
# collapse red mt and organ mt to same group give very low intake of organ mts
xdata$Meat<-xdata$RedMts + xdata$OrganMts
# copy
x.data <- xdata
# function
enet_pat<-function(xmat,yvec,wts,plot.title){
require(glmnet)
require(latex2exp)
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
store<-list()
coefsdt<-list()
for (i in 1:length(seq(0,1,by=0.1))){ # set the grid of alpha values
set.seed(28)
enetr<-cv.glmnet(x=xmat,y=yvec,family='binomial',weights = wts,
nfold=10,alpha =seq(0,1,by=0.1)[i])
evm<-data.frame(cbind(enetr$lambda,enetr$cvm))
colnames(evm)<-c('lambda','av.error')
evm[which(evm$av.error==min(evm$av.error)),]
# now create a list that stores each coefficients matrix for each value of alpha
# at the lambda minimizer
coefsdt[[i]]<-list(alpha=paste0(seq(0,1,by=0.1))[i],coefs=coef(enetr, s = "lambda.min"))
# create a dataframe that houses the alpha, min lambda, and average error
resdf<-data.frame(alpha=seq(0,1,by=0.1)[i],evm[which(evm$av.error==min(evm$av.error)),'lambda'],
av.error=evm[which(evm$av.error==min(evm$av.error)),'av.error'])
colnames(resdf)<-c('alpha','lambda','av.error')
store[[i]]<-resdf
# generate plot
if (i == 1){ # for the first value of 'i'
plot(x=enetr$lambda,y=enetr$cvm,type='l',ylim=c(min(enetr$cvm)-0.02,max(enetr$cvm)-0.02),
xlim=c(min(evm$lambda),(resdf$lambda*1.05)),las=0,cex.axis=0.7)
}
else if (i !=1){ # each additional line will be superimposed on the plot with a different color
lines(x=enetr$lambda,y=enetr$cvm, col=colorss[i])
}
}
(cverr<-do.call('rbind',store)) # this gives the table of errors for each combination of alpha and lambda
abline(h=cverr[which(cverr$av.error==min(cverr$av.error)),'av.error'],
lty=2)
abline(v=cverr[which(cverr$av.error==min(cverr$av.error)),'lambda'],
lty=2)
optimall<-cverr[which(cverr$av.error==min(cverr$av.error)),] # here I extract the optimal combination of
#lambda and alpha
optlam<-signif(optimall[2],2)
opta<-optimall[1]
title(main = TeX(paste0(plot.title,' ($\\lambda_{optimal}=$',optlam,' and $\\alpha_{optimal}=$',opta,')')))
title(xlab=TeX('$\\lambda$'),mgp=c(2,1,0))
title(ylab='Deviance',mgp=c(2,1,0))
# the function returns the optimal lambda alpha combo and the set of coefficients that
# correspond to that combination of parameters
return(list(optimall,coefs=as.matrix(coefsdt[[which(seq(0,1,by=0.1)==optimall$alpha)]]$coefs)[-1,]))
}
#### SUBSET DATA MATRIX FOR ELASTIC NET PROCEDURE
caonly<-x.data[which(x.data$Diet_ext_ind_reg==1),]
caonly<-caonly%>%
mutate(FoodAsstPnowic=ifelse(FoodAsstPnowic=='yes',1,
ifelse(FoodAsstPnowic=='no',0,NA)))%>%
mutate(Agecat=ifelse(Agecat=='elderly',1,
ifelse(Agecat=='non-elderly',0,NA)))%>%
mutate(BinFSH=ifelse(BinFoodSecHH=='Low',1,
ifelse(BinFoodSecHH=='High',0,NA)))%>%
mutate(HHSize_bin=ifelse(HHSize>=5,1,
ifelse(HHSize<5,0,NA)))
fdgrp.columns<-which(colnames(caonly) %in% c('ProcessedMts','Meat','Poultry','Fish_Hi','Fish_Lo',
'Eggs','SolidFats','Oils','Milk','Yogurt','Cheese',
'Alcohol','FruitOther','F_CitMelBer','Tomatoes',
'GreenLeafy',
'DarkYlVeg','OtherVeg',
'Potatoes','OtherStarchyVeg',
'Legumes','Soy','RefinedGrain','WholeGrain','Nuts',
'AddedSugars'))
fdgrp.columns<-fdgrp.columns[c(1,26,2:25)] # re-arrange so that Meat column index is second column index
fs.outcome.column<-which(colnames(caonly)=='BinFSH')
fdas.outcome.column<-which(colnames(caonly)=='FoodAsstPnowic')
age.outcome.column<-which(colnames(caonly)=='Agecat')
hhsize.outcome.column<-which(colnames(caonly)=='HHSize_bin')
weight.column<-which(colnames(caonly)=='WTDR18YR')
kcal.column<-which(colnames(caonly)=='KCAL')
seqn.column<-which(colnames(caonly)=='SEQN')
### ADJUSTMENT FOR TOTAL ENERGY INTAKE PRIOR TO EXTRACTION ###
# divide by total energy intake for multivariate density approach to energy adjustment
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-caonly[,j]/caonly[,kcal.column]
}
# center and scale food group variables before regressions
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-(caonly[,j]-mean(caonly[,j],na.rm=T))/sd(caonly[,j],na.rm = T)
}
# FS outcome
par(mfrow=c(2,2),mar=c(3,3,2,1))
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fs.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
fsoc<-enet_pat(xmat,yvec,xwts,plot.title='Food Insecurity') ###fix here
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
# function
enet_pat<-function(xmat,yvec,wts,plot.title){
require(glmnet)
require(latex2exp)
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
store<-list()
coefsdt<-list()
for (i in 1:length(seq(0,1,by=0.1))){ # set the grid of alpha values
set.seed(28)
enetr<-cv.glmnet(x=xmat,y=yvec,family='binomial',weights = wts,
nfold=10,alpha =seq(0,1,by=0.1)[i])
evm<-data.frame(cbind(enetr$lambda,enetr$cvm))
colnames(evm)<-c('lambda','av.error')
evm[which(evm$av.error==min(evm$av.error)),]
# now create a list that stores each coefficients matrix for each value of alpha
# at the lambda minimizer
coefsdt[[i]]<-list(alpha=paste0(seq(0,1,by=0.1))[i],coefs=coef(enetr, s = "lambda.min"))
# create a dataframe that houses the alpha, min lambda, and average error
resdf<-data.frame(alpha=seq(0,1,by=0.1)[i],evm[which(evm$av.error==min(evm$av.error)),'lambda'],
av.error=evm[which(evm$av.error==min(evm$av.error)),'av.error'])
colnames(resdf)<-c('alpha','lambda','av.error')
store[[i]]<-resdf
# generate plot
if (i == 1){ # for the first value of 'i'
plot(x=enetr$lambda,y=enetr$cvm,type='l',ylim=c(min(enetr$cvm)-0.02,max(enetr$cvm)-0.02),
xlim=c(min(evm$lambda),(resdf$lambda*1.05)),las=0,cex.axis=0.7)
}
else if (i !=1){ # each additional line will be superimposed on the plot with a different color
lines(x=enetr$lambda,y=enetr$cvm, col=colorss[i])
}
}
(cverr<-do.call('rbind',store)) # this gives the table of errors for each combination of alpha and lambda
abline(h=cverr[which(cverr$av.error==min(cverr$av.error)),'av.error'],
lty=2)
abline(v=cverr[which(cverr$av.error==min(cverr$av.error)),'lambda'],
lty=2)
optimall<-cverr[which(cverr$av.error==min(cverr$av.error)),] # here I extract the optimal combination of
#lambda and alpha
optlam<-signif(optimall[2],2)
opta<-optimall[1]
title(main = TeX(paste0(plot.title,' ($\\lambda_{optimal}=$',optlam,' and $\\alpha_{optimal}=$',opta,')')))
title(xlab=TeX('$\\lambda$'),mgp=c(2,1,0))
title(ylab='Deviance',mgp=c(2,1,0))
# the function returns the optimal lambda alpha combo and the set of coefficients that
# correspond to that combination of parameters
return(list(optimall,coefs=as.matrix(coefsdt[[which(seq(0,1,by=0.1)==optimall$alpha)]]$coefs)[-1,]))
}
#### SUBSET DATA MATRIX FOR ELASTIC NET PROCEDURE
caonly<-x.data[which(x.data$Diet_ext_ind_reg==1),]
caonly<-caonly%>%
mutate(FoodAsstPnowic=ifelse(FoodAsstPnowic=='yes',1,
ifelse(FoodAsstPnowic=='no',0,NA)))%>%
mutate(Agecat=ifelse(Agecat=='elderly',1,
ifelse(Agecat=='non-elderly',0,NA)))%>%
mutate(BinFSH=ifelse(BinFoodSecHH=='Low',1,
ifelse(BinFoodSecHH=='High',0,NA)))%>%
mutate(HHSize_bin=ifelse(HHSize>=5,1,
ifelse(HHSize<5,0,NA)))
fdgrp.columns<-which(colnames(caonly) %in% c('ProcessedMts','Meat','Poultry','Fish_Hi','Fish_Lo',
'Eggs','SolidFats','Oils','Milk','Yogurt','Cheese',
'Alcohol','FruitOther','F_CitMelBer','Tomatoes',
'GreenLeafy',
'DarkYlVeg','OtherVeg',
'Potatoes','OtherStarchyVeg',
'Legumes','Soy','RefinedGrain','WholeGrain','Nuts',
'AddedSugars'))
fdgrp.columns<-fdgrp.columns[c(1,26,2:25)] # re-arrange so that Meat column index is second column index
fs.outcome.column<-which(colnames(caonly)=='BinFSH')
fdas.outcome.column<-which(colnames(caonly)=='FoodAsstPnowic')
age.outcome.column<-which(colnames(caonly)=='Agecat')
hhsize.outcome.column<-which(colnames(caonly)=='HHSize_bin')
weight.column<-which(colnames(caonly)=='WTDR18YR')
kcal.column<-which(colnames(caonly)=='KCAL')
seqn.column<-which(colnames(caonly)=='SEQN')
### ADJUSTMENT FOR TOTAL ENERGY INTAKE PRIOR TO EXTRACTION ###
# divide by total energy intake for multivariate density approach to energy adjustment
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-caonly[,j]/caonly[,kcal.column]
}
# center and scale food group variables before regressions
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-(caonly[,j]-mean(caonly[,j],na.rm=T))/sd(caonly[,j],na.rm = T)
}
# Save plot figure
setwd('/Volumes/My Passport for Mac/Arthur Lab/FPED Raw Data/Analysis files/GitHub Repository Files /NHANES_FI_CA_Diet/Figures')
# FS outcome
par(mfrow=c(2,2),mar=c(3,3,2,1))
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fs.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
fsoc<-enet_pat(xmat,yvec,xwts,plot.title='Food Insecurity') ###fix here
wts=xwts
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
store<-list()
coefsdt<-list()
for (i in 1:length(seq(0,1,by=0.1))){ # set the grid of alpha values
set.seed(28)
enetr<-cv.glmnet(x=xmat,y=yvec,family='binomial',weights = wts,
nfold=10,alpha =seq(0,1,by=0.1)[i])
evm<-data.frame(cbind(enetr$lambda,enetr$cvm))
colnames(evm)<-c('lambda','av.error')
evm[which(evm$av.error==min(evm$av.error)),]
# now create a list that stores each coefficients matrix for each value of alpha
# at the lambda minimizer
coefsdt[[i]]<-list(alpha=paste0(seq(0,1,by=0.1))[i],coefs=coef(enetr, s = "lambda.min"))
# create a dataframe that houses the alpha, min lambda, and average error
resdf<-data.frame(alpha=seq(0,1,by=0.1)[i],evm[which(evm$av.error==min(evm$av.error)),'lambda'],
av.error=evm[which(evm$av.error==min(evm$av.error)),'av.error'])
colnames(resdf)<-c('alpha','lambda','av.error')
store[[i]]<-resdf
# generate plot
if (i == 1){ # for the first value of 'i'
plot(x=enetr$lambda,y=enetr$cvm,type='l',ylim=c(min(enetr$cvm)-0.02,max(enetr$cvm)-0.02),
xlim=c(min(evm$lambda),(resdf$lambda*1.05)),las=0,cex.axis=0.7)
}
else if (i !=1){ # each additional line will be superimposed on the plot with a different color
lines(x=enetr$lambda,y=enetr$cvm, col=colorss[i])
}
}
set.seed(28)
enetr<-cv.glmnet(x=xmat,y=yvec,family='binomial',weights = wts,
nfold=10,alpha =seq(0,1,by=0.1)[i])
i=1
seq(0,1,by=0.1)[i]
enetr<-cv.glmnet(x=xmat,y=yvec,family='binomial',weights = wts,
nfold=10,alpha =seq(0,1,by=0.1)[i])
xmat
#### SUBSET DATA MATRIX FOR ELASTIC NET PROCEDURE
caonly<-x.data[which(x.data$Diet.ext.ind.reg==1),]
caonly<-caonly%>%
mutate(FoodAsstPnowic=ifelse(FoodAsstPnowic=='yes',1,
ifelse(FoodAsstPnowic=='no',0,NA)))%>%
mutate(Agecat=ifelse(Agecat=='elderly',1,
ifelse(Agecat=='non-elderly',0,NA)))%>%
mutate(BinFSH=ifelse(BinFoodSecHH=='Low',1,
ifelse(BinFoodSecHH=='High',0,NA)))%>%
mutate(HHSize_bin=ifelse(HHSize>=5,1,
ifelse(HHSize<5,0,NA)))
fdgrp.columns<-which(colnames(caonly) %in% c('ProcessedMts','Meat','Poultry','Fish_Hi','Fish_Lo',
'Eggs','SolidFats','Oils','Milk','Yogurt','Cheese',
'Alcohol','FruitOther','F_CitMelBer','Tomatoes',
'GreenLeafy',
'DarkYlVeg','OtherVeg',
'Potatoes','OtherStarchyVeg',
'Legumes','Soy','RefinedGrain','WholeGrain','Nuts',
'AddedSugars'))
fdgrp.columns<-fdgrp.columns[c(1,26,2:25)] # re-arrange so that Meat column index is second column index
fs.outcome.column<-which(colnames(caonly)=='BinFSH')
fdas.outcome.column<-which(colnames(caonly)=='FoodAsstPnowic')
age.outcome.column<-which(colnames(caonly)=='Agecat')
hhsize.outcome.column<-which(colnames(caonly)=='HHSize_bin')
weight.column<-which(colnames(caonly)=='WTDR18YR')
kcal.column<-which(colnames(caonly)=='KCAL')
seqn.column<-which(colnames(caonly)=='SEQN')
### ADJUSTMENT FOR TOTAL ENERGY INTAKE PRIOR TO EXTRACTION ###
# divide by total energy intake for multivariate density approach to energy adjustment
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-caonly[,j]/caonly[,kcal.column]
}
# center and scale food group variables before regressions
for (j in fdgrp.columns){#ensure proper variables are indicated by the column index in this line of code before proceeding
caonly[,j]<-(caonly[,j]-mean(caonly[,j],na.rm=T))/sd(caonly[,j],na.rm = T)
}
# FS outcome
par(mfrow=c(2,2),mar=c(3,3,2,1))
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fs.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
fsoc<-enet_pat(xmat,yvec,xwts,plot.title='Food Insecurity') ###fix here
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
legend("topright", # Add legend to plot
legend = c(paste(seq(0,1,by=0.1)),'Minimizer'), col = c(colorss,'grey'),
lty = c(rep(1,11),2),title=TeX('$\\alpha$'),
cex=0.45,inset=0,y.intersp=0.5)
# Age outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,age.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
ageoc<-enet_pat(xmat,yvec,xwts,plot.title = 'Age')
fsoc$coefs
# FoodAsst outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fdas.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
fdasoc<-enet_pat(xmat,yvec,xwts,plot.title = 'Food Assistance (SNAP)')
# Reciept of food assistance outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fdas.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
# HHSize outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,hhsize.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
hhssoc<-enet_pat(xmat,yvec,xwts,plot.title='Household Size')
lambda.grid <- seq(0, 0.01, length=10)
alpha.grid <- seq(0, 1, 0.1)
train.grid <- expand.grid(.lambda = lambda.grid,.alpha = alpha.grid)
get_elastic <- function(x, y, wts, cv.type='cv', nfold=10, grid, error.type){
ctrl    <- trainControl(method = cv.type,
number = nfold,
savePredictions = TRUE,
verboseIter = T)
mod_fit <- train(y= factor(yvec),
x=xmat,
tuneGrid = grid,
method = "glmnet",
family="binomial",
trControl = ctrl, type.measure= error.type,weights = xwts)
return(as.matrix(coef(mod_fit$finalModel, mod_fit$bestTune$alpha))[-1,])
}
# Age outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,age.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
ageoc<-enet_pat(xmat,yvec,xwts,plot.title = 'Age')
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fs.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
get_elastic(x = xmat, y= yvec, grid=train.grid, error.type = 'auc')
setwd("/Volumes/My Passport for Mac/Arthur Lab/FPED Raw Data/Analysis files/GitHub Repository Files /NHANES-Diet-Penalized-Regression/Manuscript/Tables")
# Save plot figure
setwd("/Volumes/My Passport for Mac/Arthur Lab/FPED Raw Data/Analysis files/GitHub Repository Files /NHANES-Diet-Penalized-Regression/Manuscript/Figures")
pdf("optimal_lambdas_enet_patterns.pdf")
# FS outcome
par(mfrow=c(2,2),mar=c(3,3,2,1))
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,fs.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
fsoc<-enet_pat(xmat,yvec,xwts,plot.title='Food Insecurity') ###fix here
colorss<-c("black", "red","green3", "navyblue",   "cyan",   "magenta","gold", "gray",
'pink','brown','goldenrod')
legend("topright", # Add legend to plot
legend = c(paste(seq(0,1,by=0.1)),'Minimizer'), col = c(colorss,'grey'),
lty = c(rep(1,11),2),title=TeX('$\\alpha$'),
cex=0.45,inset=0,y.intersp=0.5)
# Age outcome
mat<-na.omit(as.matrix(caonly[,c(fdgrp.columns,age.outcome.column,weight.column)]))
xmat<-mat[,1:26] # food grps
yvec<-mat[,27] # binary response
xwts<-mat[,28]/mean(mat[,28]) # normalize weights
ageoc<-enet_pat(xmat,yvec,xwts,plot.title = 'Age')
